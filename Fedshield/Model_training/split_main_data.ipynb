{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"--- Part 1: Splitting the Main Dataset (Memory-Efficient & Safe) ---\")\n",
    "\n",
    "# --- Configuration with your specific paths ---\n",
    "BASE_DIR = r\"D:\\FedShield,Personal\\wataiData\\csv\\CICIoT2023\"\n",
    "FULL_DATASET_PATH = os.path.join(BASE_DIR, 'full_dataset.csv')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'federated_data')\n",
    "\n",
    "# --- Other Configuration ---\n",
    "SERVER_DATASET_SIZE = 0.10  # 10% for the initial global model\n",
    "CHUNK_SIZE = 100000         # Process 100,000 rows at a time\n",
    "\n",
    "# --- Create Output Directory ---\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# --- Define Output File Paths ---\n",
    "server_output_path = os.path.join(OUTPUT_DIR, 'server_df.csv')\n",
    "clients_output_path = os.path.join(OUTPUT_DIR, 'clients_df.csv')\n",
    "\n",
    "# --- Remove old files if they exist ---\n",
    "if os.path.exists(server_output_path):\n",
    "    os.remove(server_output_path)\n",
    "if os.path.exists(clients_output_path):\n",
    "    os.remove(clients_output_path)\n",
    "\n",
    "# --- Process the large CSV in chunks ---\n",
    "print(f\"Loading and processing {FULL_DATASET_PATH} in chunks...\")\n",
    "header_written = False\n",
    "chunk_num = 1\n",
    "reader = pd.read_csv(FULL_DATASET_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "for chunk in reader:\n",
    "    print(f\"  -> Processing chunk {chunk_num}...\")\n",
    "    chunk.dropna(inplace=True)\n",
    "\n",
    "    if chunk.empty:\n",
    "        chunk_num += 1\n",
    "        continue\n",
    "    \n",
    "    # *** START OF THE FIX ***\n",
    "    # Check the counts of each label in the current chunk\n",
    "    label_counts = chunk['label'].value_counts()\n",
    "    \n",
    "    # If any label has fewer than 2 samples, stratification is not possible\n",
    "    if (label_counts < 2).any():\n",
    "        print(f\"     ! Chunk {chunk_num} has a rare label with only 1 sample. Assigning full chunk to clients_df to avoid error.\")\n",
    "        # In this rare case, we can't split, so we'll just append the whole chunk to the larger clients file\n",
    "        if not header_written:\n",
    "             clients_chunk.to_csv(clients_output_path, index=False, mode='w', header=True)\n",
    "             # We still need a header for the server file, so we write an empty dataframe with the correct columns\n",
    "             pd.DataFrame(columns=chunk.columns).to_csv(server_output_path, index=False, mode='w', header=True)\n",
    "             header_written = True\n",
    "        else:\n",
    "             chunk.to_csv(clients_output_path, index=False, mode='a', header=False)\n",
    "        \n",
    "        chunk_num += 1\n",
    "        continue # Skip to the next chunk\n",
    "    # *** END OF THE FIX ***\n",
    "\n",
    "    # If the check passes, split the chunk as normal\n",
    "    server_chunk, clients_chunk = train_test_split(\n",
    "        chunk,\n",
    "        test_size=(1 - SERVER_DATASET_SIZE),\n",
    "        random_state=42,\n",
    "        stratify=chunk['label']\n",
    "    )\n",
    "\n",
    "    if not header_written:\n",
    "        server_chunk.to_csv(server_output_path, index=False, mode='w', header=True)\n",
    "        clients_chunk.to_csv(clients_output_path, index=False, mode='w', header=True)\n",
    "        header_written = True\n",
    "    else:\n",
    "        server_chunk.to_csv(server_output_path, index=False, mode='a', header=False)\n",
    "        clients_chunk.to_csv(clients_output_path, index=False, mode='a', header=False)\n",
    "    \n",
    "    chunk_num += 1\n",
    "\n",
    "print(f\"\\nâœ… Part 1 Complete!\")\n",
    "print(f\"Data has been split and saved chunk by chunk.\")\n",
    "print(f\"Initial server data saved to: {server_output_path}\")\n",
    "print(f\"Combined client data saved to: {clients_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
